rm(list = ls())       #清除右邊視窗變數
cat('\014')
#使用combine函數c()，將這五篇文章合併為一個文字向量vdocs
myDoc=c("A dog barks at a cat and it fell from a 1-Tree.",
"A dog watches ants on the bark of a 2-Tree.",
"A dog watches another dog watches a 3Cat.",
"A dog barks at a cat watches another 4Cat.",
"The bark fell from the 5-tree as a cat Watches.")
#R中顯示向量內容的基本方法
myDoc
myDoc[1]
myDoc[3:5]
myDoc[c(2,4)]
#安裝tm套件
#install.packages("tm")
#載入tm套件
library(tm)
getSources() #告訴您R的語料庫來源形式可以是哪些哪些
getReaders() #告訴您R可以讀的格式有哪些哪些
cp_myDoc=Corpus(VectorSource(myDoc))
cp_myDoc
#在R中語料庫Corpus的是屬於List資料型態所以無法直接點選看內容
#可以使用inspect()函數查看內容
inspect(cp_myDoc)
#直接點選只會顯示此Corpus的概述
cp_myDoc
#看一下右邊視窗中這一個Corpus變數的組成
#可以使用$符號接content屬性看內容
cp_myDoc$content
cp_myDoc$content[1]  #看任意筆資料
#或者可以使用[[]]兩個中括號來查看List資料
cp_myDoc[[1]]
cp_myDoc[[1]][1]  #就是content真實內容
#試著先將目前語料庫的所有英文字透過DocumentTermMatrix函數
#產生一個文件(檔)-詞條矩陣通常以DTM or dtm命名
DTM=DocumentTermMatrix(cp_myDoc)
#該矩陣依然為List資料型態，只能使用inspect()函數查看內容
inspect(DTM)
#另外也可以使用$符號接dimnames屬性以及Docs or Terms屬性
#看內容
DTM$dimnames$Docs
DTM$dimnames$Terms
#同理，使用TermDocumentMatrix()函數產生詞條文件矩陣，
#命名方式常為TDM or tdm
TDM = TermDocumentMatrix(cp_myDoc)
inspect(TDM)
#找出發生5次以上的字詞(條目)
findFreqTerms(DTM,5)
findFreqTerms(TDM,5)
#或者可以找相關性，例如針對dog，找到相關係數在0.6以上的字詞(條目)
findAssocs(DTM, "dog", 0.6)
findAssocs(TDM, "dog", 0.6)
#英文文件探勘的資料前處理工作重點有兩個
#(1)stopwords停用字的移除工作
#(2)stemming詞幹化還原為最原始的字
#所以在產生DTM 或 TDM之前應該要先移除stopword
#以及做stemming工作，
#因此我們必須先看看目前語料庫的樣子跟等一下去除後比對一下
cp_myDoc$content
#這兩項工作都可以由tm套件中tm_map()函數來協助
#tm_map函數先全部轉換成小寫英文字母
cp_myDoc=tm_map(cp_myDoc,content_transformer(tolower))
#比較看看語料庫的樣子
cp_myDoc$content
cp_myDoc
#tm_map函數將數字0~9移除掉
cp_myDoc=tm_map(cp_myDoc,removeNumbers)
removePunctuation
#tm_map函數去掉不需要的標點符號
cp_myDoc=tm_map(cp_myDoc,removePunctuation)
#再看看語料庫中標點符號是否已經去掉呢?
cp_myDoc$content
#在tm套件中stopwords()函數顯示R的詞庫中認為對於分析無幫助，
#應該去掉的英文字stopwords(稱為停用字)有哪些?總共有174個
stopwords()
#先看看尚未去掉停用字之前語料庫的情況
cp_myDoc$content
cp_myDoc[[1]]$content
#tm_map函數可以從語料庫中去掉停用詞中所條列的不需要的字
cp_myDoc=tm_map(cp_myDoc,removeWords,stopwords())
#再看一次已經去掉停用字之後語料庫的情況
cp_myDoc$content
cp_myDoc[[1]]$content
install.packages("SnowballC")   #安裝SnowballC套件
library(SnowballC)              #載入SnowballC套件
#先看看Stemming工作之前barks在句子在哪裡
cp_myDoc$content
cp_myDoc[[4]]$content
#執行Stemming工作(詞幹化工作)
cp_myDoc=tm_map(cp_myDoc,stemDocument)
cp_myDoc$content
hw1 <- read.table('https://github.com/1112-nccu-datascience/hw1-yuu0223/blob/main/example/input1.csv',
sep = ',',
header = T)
getwd()
setwd("./GitHub/hw2-yuu0223/examples/")
View(output.csv)
View(output1.csv)
output1<-read.csv("output1.csv")
View(output1)
