{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["ZqS84cDCHzlb"],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i6MBVRSro5tt","outputId":"0265a230-943f-4edc-a987-726641684d42","executionInfo":{"status":"ok","timestamp":1713880282101,"user_tz":-480,"elapsed":20544,"user":{"displayName":"陳品伃","userId":"10060191056859703941"}}},"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","\n","import nltk\n","from nltk.corpus import movie_reviews\n","from nltk.tokenize import word_tokenize\n","nltk.download(\"movie_reviews\")\n","\n","from collections import defaultdict, Counter\n","import math\n","import random\n","\n","random.seed(0) # Don't change\n","torch.manual_seed(0)  # Don't change\n","np.random.seed(0) # Don't change\n","\n","\n","train_X, train_Y = [], []\n","test_X, test_Y = [], []\n","\n","for polarity in movie_reviews.categories():\n","    label = 0 if polarity == 'neg' else 1\n","    for fid in movie_reviews.fileids(polarity):\n","        if random.randrange(5) == 0:\n","            test_X.append([w for w in movie_reviews.words(fid)])\n","            test_Y.append(label)\n","        else:\n","            train_X.append([w for w in movie_reviews.words(fid)])\n","            train_Y.append(label)\n","\n","print(train_X[0], train_Y[0])"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/movie_reviews.zip.\n"]},{"output_type":"stream","name":"stdout","text":["['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', ',', 'drink', 'and', 'then', 'drive', '.', 'they', 'get', 'into', 'an', 'accident', '.', 'one', 'of', 'the', 'guys', 'dies', ',', 'but', 'his', 'girlfriend', 'continues', 'to', 'see', 'him', 'in', 'her', 'life', ',', 'and', 'has', 'nightmares', '.', 'what', \"'\", 's', 'the', 'deal', '?', 'watch', 'the', 'movie', 'and', '\"', 'sorta', '\"', 'find', 'out', '.', '.', '.', 'critique', ':', 'a', 'mind', '-', 'fuck', 'movie', 'for', 'the', 'teen', 'generation', 'that', 'touches', 'on', 'a', 'very', 'cool', 'idea', ',', 'but', 'presents', 'it', 'in', 'a', 'very', 'bad', 'package', '.', 'which', 'is', 'what', 'makes', 'this', 'review', 'an', 'even', 'harder', 'one', 'to', 'write', ',', 'since', 'i', 'generally', 'applaud', 'films', 'which', 'attempt', 'to', 'break', 'the', 'mold', ',', 'mess', 'with', 'your', 'head', 'and', 'such', '(', 'lost', 'highway', '&', 'memento', ')', ',', 'but', 'there', 'are', 'good', 'and', 'bad', 'ways', 'of', 'making', 'all', 'types', 'of', 'films', ',', 'and', 'these', 'folks', 'just', 'didn', \"'\", 't', 'snag', 'this', 'one', 'correctly', '.', 'they', 'seem', 'to', 'have', 'taken', 'this', 'pretty', 'neat', 'concept', ',', 'but', 'executed', 'it', 'terribly', '.', 'so', 'what', 'are', 'the', 'problems', 'with', 'the', 'movie', '?', 'well', ',', 'its', 'main', 'problem', 'is', 'that', 'it', \"'\", 's', 'simply', 'too', 'jumbled', '.', 'it', 'starts', 'off', '\"', 'normal', '\"', 'but', 'then', 'downshifts', 'into', 'this', '\"', 'fantasy', '\"', 'world', 'in', 'which', 'you', ',', 'as', 'an', 'audience', 'member', ',', 'have', 'no', 'idea', 'what', \"'\", 's', 'going', 'on', '.', 'there', 'are', 'dreams', ',', 'there', 'are', 'characters', 'coming', 'back', 'from', 'the', 'dead', ',', 'there', 'are', 'others', 'who', 'look', 'like', 'the', 'dead', ',', 'there', 'are', 'strange', 'apparitions', ',', 'there', 'are', 'disappearances', ',', 'there', 'are', 'a', 'looooot', 'of', 'chase', 'scenes', ',', 'there', 'are', 'tons', 'of', 'weird', 'things', 'that', 'happen', ',', 'and', 'most', 'of', 'it', 'is', 'simply', 'not', 'explained', '.', 'now', 'i', 'personally', 'don', \"'\", 't', 'mind', 'trying', 'to', 'unravel', 'a', 'film', 'every', 'now', 'and', 'then', ',', 'but', 'when', 'all', 'it', 'does', 'is', 'give', 'me', 'the', 'same', 'clue', 'over', 'and', 'over', 'again', ',', 'i', 'get', 'kind', 'of', 'fed', 'up', 'after', 'a', 'while', ',', 'which', 'is', 'this', 'film', \"'\", 's', 'biggest', 'problem', '.', 'it', \"'\", 's', 'obviously', 'got', 'this', 'big', 'secret', 'to', 'hide', ',', 'but', 'it', 'seems', 'to', 'want', 'to', 'hide', 'it', 'completely', 'until', 'its', 'final', 'five', 'minutes', '.', 'and', 'do', 'they', 'make', 'things', 'entertaining', ',', 'thrilling', 'or', 'even', 'engaging', ',', 'in', 'the', 'meantime', '?', 'not', 'really', '.', 'the', 'sad', 'part', 'is', 'that', 'the', 'arrow', 'and', 'i', 'both', 'dig', 'on', 'flicks', 'like', 'this', ',', 'so', 'we', 'actually', 'figured', 'most', 'of', 'it', 'out', 'by', 'the', 'half', '-', 'way', 'point', ',', 'so', 'all', 'of', 'the', 'strangeness', 'after', 'that', 'did', 'start', 'to', 'make', 'a', 'little', 'bit', 'of', 'sense', ',', 'but', 'it', 'still', 'didn', \"'\", 't', 'the', 'make', 'the', 'film', 'all', 'that', 'more', 'entertaining', '.', 'i', 'guess', 'the', 'bottom', 'line', 'with', 'movies', 'like', 'this', 'is', 'that', 'you', 'should', 'always', 'make', 'sure', 'that', 'the', 'audience', 'is', '\"', 'into', 'it', '\"', 'even', 'before', 'they', 'are', 'given', 'the', 'secret', 'password', 'to', 'enter', 'your', 'world', 'of', 'understanding', '.', 'i', 'mean', ',', 'showing', 'melissa', 'sagemiller', 'running', 'away', 'from', 'visions', 'for', 'about', '20', 'minutes', 'throughout', 'the', 'movie', 'is', 'just', 'plain', 'lazy', '!', '!', 'okay', ',', 'we', 'get', 'it', '.', '.', '.', 'there', 'are', 'people', 'chasing', 'her', 'and', 'we', 'don', \"'\", 't', 'know', 'who', 'they', 'are', '.', 'do', 'we', 'really', 'need', 'to', 'see', 'it', 'over', 'and', 'over', 'again', '?', 'how', 'about', 'giving', 'us', 'different', 'scenes', 'offering', 'further', 'insight', 'into', 'all', 'of', 'the', 'strangeness', 'going', 'down', 'in', 'the', 'movie', '?', 'apparently', ',', 'the', 'studio', 'took', 'this', 'film', 'away', 'from', 'its', 'director', 'and', 'chopped', 'it', 'up', 'themselves', ',', 'and', 'it', 'shows', '.', 'there', 'might', \"'\", 've', 'been', 'a', 'pretty', 'decent', 'teen', 'mind', '-', 'fuck', 'movie', 'in', 'here', 'somewhere', ',', 'but', 'i', 'guess', '\"', 'the', 'suits', '\"', 'decided', 'that', 'turning', 'it', 'into', 'a', 'music', 'video', 'with', 'little', 'edge', ',', 'would', 'make', 'more', 'sense', '.', 'the', 'actors', 'are', 'pretty', 'good', 'for', 'the', 'most', 'part', ',', 'although', 'wes', 'bentley', 'just', 'seemed', 'to', 'be', 'playing', 'the', 'exact', 'same', 'character', 'that', 'he', 'did', 'in', 'american', 'beauty', ',', 'only', 'in', 'a', 'new', 'neighborhood', '.', 'but', 'my', 'biggest', 'kudos', 'go', 'out', 'to', 'sagemiller', ',', 'who', 'holds', 'her', 'own', 'throughout', 'the', 'entire', 'film', ',', 'and', 'actually', 'has', 'you', 'feeling', 'her', 'character', \"'\", 's', 'unraveling', '.', 'overall', ',', 'the', 'film', 'doesn', \"'\", 't', 'stick', 'because', 'it', 'doesn', \"'\", 't', 'entertain', ',', 'it', \"'\", 's', 'confusing', ',', 'it', 'rarely', 'excites', 'and', 'it', 'feels', 'pretty', 'redundant', 'for', 'most', 'of', 'its', 'runtime', ',', 'despite', 'a', 'pretty', 'cool', 'ending', 'and', 'explanation', 'to', 'all', 'of', 'the', 'craziness', 'that', 'came', 'before', 'it', '.', 'oh', ',', 'and', 'by', 'the', 'way', ',', 'this', 'is', 'not', 'a', 'horror', 'or', 'teen', 'slasher', 'flick', '.', '.', '.', 'it', \"'\", 's', 'just', 'packaged', 'to', 'look', 'that', 'way', 'because', 'someone', 'is', 'apparently', 'assuming', 'that', 'the', 'genre', 'is', 'still', 'hot', 'with', 'the', 'kids', '.', 'it', 'also', 'wrapped', 'production', 'two', 'years', 'ago', 'and', 'has', 'been', 'sitting', 'on', 'the', 'shelves', 'ever', 'since', '.', 'whatever', '.', '.', '.', 'skip', 'it', '!', 'where', \"'\", 's', 'joblo', 'coming', 'from', '?', 'a', 'nightmare', 'of', 'elm', 'street', '3', '(', '7', '/', '10', ')', '-', 'blair', 'witch', '2', '(', '7', '/', '10', ')', '-', 'the', 'crow', '(', '9', '/', '10', ')', '-', 'the', 'crow', ':', 'salvation', '(', '4', '/', '10', ')', '-', 'lost', 'highway', '(', '10', '/', '10', ')', '-', 'memento', '(', '10', '/', '10', ')', '-', 'the', 'others', '(', '9', '/', '10', ')', '-', 'stir', 'of', 'echoes', '(', '8', '/', '10', ')'] 0\n"]}]},{"cell_type":"markdown","source":["# Assignment II\n","Doing Assignment II by modifying the following code cell.\n","Your solution should be based on feedforward neural network (FNN or MLP) with word embeddings.\n","You are free to adjust the FNN with different dimension settings, vocabulary, overfitting prevention, and so on,\n","but you can not use other architectures (e.g., CNN/RNN/Transformer or the Naive Bayes classifier from Assignment I) in this assignment.\n"],"metadata":{"id":"ds984vYgcX7V"}},{"cell_type":"code","source":["from nltk.tokenize import word_tokenize\n","from nltk.stem import PorterStemmer, WordNetLemmatizer\n","nltk.download('wordnet')\n","import spacy\n","from nltk import ngrams\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","EMBEDDING_DIM = 450\n","EPOCHS = 12\n","\n","\n","### Stop Words\n","nlp = spacy.load('en_core_web_sm')\n","stop_words = spacy.lang.en.stop_words.STOP_WORDS\n","### Lemmatize\n","lemmatizer = WordNetLemmatizer()\n","\n","\n","class TextClassifier(nn.Module):\n","  def init_embeddings(self, vocab):\n","    self.word_to_ix = {}\n","    weights = []\n","    ix = 0\n","    for w in vocab:\n","      self.word_to_ix[w] = ix\n","      ix += 1\n","    self.vocab_size = len(self.word_to_ix)\n","    self.embeddings = nn.EmbeddingBag(self.vocab_size, EMBEDDING_DIM)\n","\n","  def __init__(self, vocab, classes):\n","    super(TextClassifier, self).__init__()\n","    self.classes = classes\n","    self.init_embeddings(vocab)\n","    self.fc1 = nn.Linear(self.embeddings.embedding_dim, 50)\n","    self.fc1.weight.data.uniform_(-0.5, 0.5)\n","    self.fc1.bias.data.zero_()\n","    self.fc2 = nn.Linear(50, 20)\n","    self.fc2.weight.data.uniform_(-0.5, 0.5)\n","    self.fc2.bias.data.zero_()\n","    self.out = nn.Linear(20, len(self.classes))\n","    self.out.weight.data.uniform_(-0.5, 0.5)\n","    self.out.bias.data.zero_()\n","    self.relu = nn.ReLU()\n","\n","  def forward(self, inputs, offsets):\n","    embedded = self.embeddings(inputs, offsets)\n","    return self.out(self.relu(self.fc2(self.relu(self.fc1(embedded)))))\n","\n","\n","def preprocess(tokens, ngram_range):\n","  # tokens = [token for token in tokens if token not in stop_words]\n","  tokens = [token.lower() for token in tokens]\n","  tokens = [lemmatizer.lemmatize(token) for token in tokens]\n","  ngram_tokens = []\n","  for n in range(ngram_range[0], ngram_range[1]+1):\n","      ngrams_n = ngrams(tokens, n)\n","      ngram_tokens.extend(['_'.join(grams) for grams in ngrams_n])\n","  return ngram_tokens\n","\n","def make_doc_vector(doc, word_to_ix):\n","  doc = preprocess(doc, ngram_range=(1,3))\n","  idxs = [word_to_ix[w] for w in doc if w in word_to_ix]\n","  return torch.tensor(idxs, dtype=torch.long)\n","\n","def generate_batch(batch):\n","  label = torch.tensor([entry[0] for entry in batch])\n","  text = [entry[1] for entry in batch]\n","  offsets = [0] + [len(entry) for entry in text]\n","  offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n","  text = torch.cat(text)\n","  return text, offsets, label\n","\n","def build_vocab(X, ngram_range=(1,3)):\n","  word_count = Counter()\n","  for x in X:\n","    doc = preprocess(x, ngram_range)\n","    for w in x:\n","      word_count[w] += 1\n","  # The order of keys in a dictionary/set is not deterministic,\n","  # so sorting in the following statement is important to avoid randomness.\n","  return [w for (w, c) in sorted(word_count.items()) if c >= 10]\n","\n","def build_model(X, Y):\n","  model = TextClassifier(build_vocab(X), [0, 1]).to(device)\n","  loss_function = nn.CrossEntropyLoss().to(device)\n","  optimizer = optim.Adam(model.parameters())\n","\n","  train_set = []\n","  yc = Counter()\n","  for x, y in zip(X, Y):\n","    entry = []\n","    yc[y] += 1\n","    entry.append(torch.LongTensor([y]))\n","    entry.append(make_doc_vector(x, model.word_to_ix))\n","    train_set.append(entry)\n","  print(yc)\n","  data = DataLoader(train_set, batch_size=32, shuffle=True, collate_fn=generate_batch)\n","\n","  for epoch in range(EPOCHS):\n","    train_loss, train_acc = 0, 0\n","    print(\"Epoch: %d\" % epoch)\n","    for _, (x, offsets, y) in enumerate(data):\n","      model.zero_grad()\n","      x, offsets, y = x.to(device), offsets.to(device), y.to(device)\n","      pred = model(x, offsets)\n","      loss = loss_function(pred, y)\n","      loss.backward()\n","      optimizer.step()\n","      train_loss += loss.item()\n","      train_acc += (pred.argmax(1) == y).sum().item()\n","    print(\"Loss: %g, Acc: %g\" % (train_loss / len(train_set), train_acc / len(train_set)))\n","  return model"],"metadata":{"id":"-E16EhQUGem8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713880288305,"user_tz":-480,"elapsed":6209,"user":{"displayName":"陳品伃","userId":"10060191056859703941"}},"outputId":"07f16661-2041-458f-8594-df8da2a49e48"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]}]},{"cell_type":"markdown","source":["### CPU-test"],"metadata":{"id":"CAcm-Mt5EBMj"}},{"cell_type":"code","source":["model = build_model(train_X, train_Y)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713880304786,"user_tz":-480,"elapsed":16501,"user":{"displayName":"陳品伃","userId":"10060191056859703941"}},"outputId":"11b7f422-008f-407e-b7e8-f86df54215e9","id":"wqRomCh9EBMk"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Counter({1: 803, 0: 775})\n","Epoch: 0\n","Loss: 0.0222247, Acc: 0.562738\n","Epoch: 1\n","Loss: 0.0183392, Acc: 0.708492\n","Epoch: 2\n","Loss: 0.0151704, Acc: 0.775665\n","Epoch: 3\n","Loss: 0.0114025, Acc: 0.856147\n","Epoch: 4\n","Loss: 0.00765353, Acc: 0.922687\n","Epoch: 5\n","Loss: 0.00490019, Acc: 0.959442\n","Epoch: 6\n","Loss: 0.00318427, Acc: 0.979087\n","Epoch: 7\n","Loss: 0.00187816, Acc: 0.993029\n","Epoch: 8\n","Loss: 0.00109686, Acc: 0.998099\n","Epoch: 9\n","Loss: 0.000685609, Acc: 0.998733\n","Epoch: 10\n","Loss: 0.00044894, Acc: 1\n","Epoch: 11\n","Loss: 0.000328015, Acc: 1\n"]}]},{"cell_type":"code","source":["def predict(model, document):\n","  probs = model(make_doc_vector(document, model.word_to_ix).to(device), torch.tensor([0]).cumsum(dim=0).to(device))\n","  return int(torch.argmax(probs))\n","\n","print(predict(model, \"this is a uninteresting movie\".split(\" \")))\n","print(predict(model, \"a good movie of this year\".split(\" \")))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713880304787,"user_tz":-480,"elapsed":15,"user":{"displayName":"陳品伃","userId":"10060191056859703941"}},"outputId":"0aa7eeb7-50c1-4220-ed78-bb3baae704a1","id":"lv0luTD6EBMl"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["0\n","0\n"]}]},{"cell_type":"code","source":["correct, total = 0, 0\n","\n","for x, y in zip(test_X, test_Y):\n","    # print(test_X)\n","    prediction = predict(model, x)\n","    if prediction == y:\n","        correct += 1\n","    total += 1\n","\n","print(\"%d / %d = %g\" % (correct, total, correct / total))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fktIFd26HPfc","executionInfo":{"status":"ok","timestamp":1713880306324,"user_tz":-480,"elapsed":1551,"user":{"displayName":"陳品伃","userId":"10060191056859703941"}},"outputId":"bcc8aff0-ff34-4c57-c104-dcad24700b45"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["343 / 422 = 0.812796\n"]}]},{"cell_type":"code","source":["correct, total = 0, 0\n","\n","for x, y in zip(test_X, test_Y):\n","    # print(test_X)\n","    prediction = predict(model, x)\n","    if prediction == y:\n","        correct += 1\n","    total += 1\n","\n","print(\"%d / %d = %g\" % (correct, total, correct / total))"],"metadata":{"id":"0FZpFEtLjvEf","executionInfo":{"status":"ok","timestamp":1713878377881,"user_tz":-480,"elapsed":763,"user":{"displayName":"陳品伃","userId":"10060191056859703941"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"6038e080-35b3-4ee7-a85b-ac1a113bd59f"},"execution_count":281,"outputs":[{"output_type":"stream","name":"stdout","text":["360 / 422 = 0.853081\n"]}]},{"cell_type":"markdown","source":["### CPU"],"metadata":{"id":"S4ZaP-jiH4LZ"}},{"cell_type":"code","source":["model = build_model(train_X, train_Y)"],"metadata":{"id":"ODbGTrFuH5pO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713877781679,"user_tz":-480,"elapsed":13540,"user":{"displayName":"陳品伃","userId":"10060191056859703941"}},"outputId":"d94052a0-a482-4c58-910a-8cdfbd0cad39"},"execution_count":196,"outputs":[{"output_type":"stream","name":"stdout","text":["Counter({1: 803, 0: 775})\n","Epoch: 0\n","Loss: 0.0110251, Acc: 0.532953\n","Epoch: 1\n","Loss: 0.0100828, Acc: 0.685044\n","Epoch: 2\n","Loss: 0.00910997, Acc: 0.726236\n","Epoch: 3\n","Loss: 0.00788481, Acc: 0.776933\n","Epoch: 4\n","Loss: 0.00650775, Acc: 0.842839\n","Epoch: 5\n","Loss: 0.00507175, Acc: 0.892902\n","Epoch: 6\n","Loss: 0.00383135, Acc: 0.940431\n","Epoch: 7\n","Loss: 0.00272833, Acc: 0.963878\n","Epoch: 8\n","Loss: 0.0019398, Acc: 0.983523\n","Epoch: 9\n","Loss: 0.00133499, Acc: 0.989861\n","Epoch: 10\n","Loss: 0.00090659, Acc: 0.995564\n","Epoch: 11\n","Loss: 0.000611742, Acc: 0.997465\n"]}]},{"cell_type":"code","source":["def predict(model, document):\n","  probs = model(make_doc_vector(document, model.word_to_ix).to(device), torch.tensor([0]).cumsum(dim=0).to(device))\n","  return int(torch.argmax(probs))\n","\n","print(predict(model, \"this is a uninteresting movie\".split(\" \")))\n","print(predict(model, \"a good movie of this year\".split(\" \")))\n"],"metadata":{"id":"f_5L6nGRH6si","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713877511575,"user_tz":-480,"elapsed":20,"user":{"displayName":"陳品伃","userId":"10060191056859703941"}},"outputId":"ebc84ffe-29a2-40f1-c180-19b109826a05"},"execution_count":170,"outputs":[{"output_type":"stream","name":"stdout","text":["0\n","0\n"]}]},{"cell_type":"markdown","metadata":{"id":"Zo1u7CAk6bvR"},"source":["## Do Evaluation"]},{"cell_type":"code","source":["correct, total = 0, 0\n","\n","for x, y in zip(test_X, test_Y):\n","    # print(test_X)\n","    prediction = predict(model, x)\n","    if prediction == y:\n","        correct += 1\n","    total += 1\n","\n","print(\"%d / %d = %g\" % (correct, total, correct / total))"],"metadata":{"id":"NNPLgEUwO0iR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713877513539,"user_tz":-480,"elapsed":1982,"user":{"displayName":"陳品伃","userId":"10060191056859703941"}},"outputId":"ff444932-ee19-4d13-fb43-2c6fbe0507ff"},"execution_count":171,"outputs":[{"output_type":"stream","name":"stdout","text":["361 / 422 = 0.85545\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"m0mud9EFg7qr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"tYwS3vQTg7sp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["correct, total = 0, 0\n","\n","for x, y in zip(test_X, test_Y):\n","    # print(test_X)\n","    prediction = predict(model, x)\n","    if prediction == y:\n","        correct += 1\n","    total += 1\n","\n","print(\"%d / %d = %g\" % (correct, total, correct / total))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eph3yR9Ma3rP","outputId":"ecfc3ca2-263c-4fdf-9dff-e5cba742ef7a","executionInfo":{"status":"ok","timestamp":1713425522934,"user_tz":-480,"elapsed":1292,"user":{"displayName":"陳品伃","userId":"10060191056859703941"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["364 / 422 = 0.862559\n"]}]},{"cell_type":"code","source":["from nltk.tokenize import word_tokenize\n","from nltk.stem import PorterStemmer, WordNetLemmatizer\n","nltk.download('wordnet')\n","import spacy\n","from gensim.models import KeyedVectors\n","import gensim.downloader as api\n","wv_model = api.load(\"word2vec-google-news-300\")\n","# wv_model = KeyedVectors.load_word2vec_format('path/to/googlenews-vectors-negative300.bin', binary=True)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","EMBEDDING_DIM = 250\n","EPOCHS = 10\n","\n","### Stop Words\n","nlp = spacy.load('en_core_web_sm')\n","stop_words = spacy.lang.en.stop_words.STOP_WORDS\n","# sign_list = [\";\", \",\", \".\", \"~\", \":\", \"-\", \"(\", \")\", \"%\", \"#\", \"$\", \"!\", \"/\", \"?\", \"=\", \"+\", \"&\", \"--\", \"'\", '\"', '`', '*']\n","# for sign in sign_list:\n","#   stop_words.add(sign)\n","### Lemmatize\n","lemmatizer = WordNetLemmatizer()\n","\n","\n","class TextClassifier(nn.Module):\n","  def init_embeddings(self, vocab):\n","    self.word_to_ix = {}\n","    weights = []\n","    ix = 0\n","    for w in vocab:\n","      self.word_to_ix[w] = ix\n","      ix += 1\n","    self.vocab_size = len(self.word_to_ix)\n","    self.embeddings = nn.EmbeddingBag(self.vocab_size, EMBEDDING_DIM)\n","\n","  def __init__(self, vocab, word_to_vec, classes):\n","    super(TextClassifier, self).__init__()\n","\n","    weights_matrix = np.zeros((len(word_to_vec), EMBEDDING_DIM))\n","    for i, word in enumerate(word_to_vec):\n","        weights_matrix[i] = word_to_vec[word]\n","    self.embeddings = nn.EmbeddingBag.from_pretrained(torch.FloatTensor(weights_matrix), freeze=False)\n","\n","    self.classes = classes\n","    self.init_embeddings(vocab)\n","    self.fc1 = nn.Linear(self.embeddings.embedding_dim, 50)\n","    self.fc1.weight.data.uniform_(-0.5, 0.5)\n","    self.fc1.bias.data.zero_()\n","    self.fc2 = nn.Linear(50, 20)\n","    self.fc2.weight.data.uniform_(-0.5, 0.5)\n","    self.fc2.bias.data.zero_()\n","    self.out = nn.Linear(20, len(self.classes))\n","    self.out.weight.data.uniform_(-0.5, 0.5)\n","    self.out.bias.data.zero_()\n","    self.relu = nn.ReLU()\n","\n","  def forward(self, inputs, offsets):\n","    embedded = self.embeddings(inputs, offsets)\n","    return self.out(self.relu(self.fc2(self.relu(self.fc1(embedded)))))\n","\n","\n","### preprocessing\n","def preprocess(tokens):\n","  tokens = [token for token in tokens if token not in stop_words]\n","  tokens = [token.lower() for token in tokens]\n","  tokens = [lemmatizer.lemmatize(token) for token in tokens]\n","  return tokens\n","\n","def make_doc_vector(doc, word_to_ix):\n","  doc = preprocess(doc)\n","  idxs = [word_to_ix[w] for w in doc if w in word_to_ix]\n","  return torch.tensor(idxs, dtype=torch.long)\n","\n","def generate_batch(batch):\n","  label = torch.tensor([entry[0] for entry in batch])\n","  text = [entry[1] for entry in batch]\n","  offsets = [0] + [len(entry) for entry in text]\n","  offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n","  text = torch.cat(text)\n","  return text, offsets, label\n","\n","def build_vocab(X):\n","  word_count = Counter()\n","  for x in X:\n","    for w in x:\n","      word_count[w] += 1\n","  # The order of keys in a dictionary/set is not deterministic,\n","  # so sorting in the following statement is important to avoid randomness.\n","  vocab = [w for (w, c) in sorted(word_count.items()) if c >= 30]\n","\n","  word_to_vec = {}\n","  for w in vocab:\n","      if w in wv_model:\n","          word_to_vec[w] = wv_model[w]\n","      else:\n","          # 對於不在Word2Vec模型中的詞，可以賦予隨機向量或零向量\n","          word_to_vec[w] = np.random.uniform(-0.25, 0.25, wv_model.vector_size)\n","\n","  return vocab, word_to_vec\n","\n","def build_model(X, Y, wv_model):\n","  vocab, word_to_vec = build_vocab(X, wv_model)\n","  model = TextClassifier(vocab, word_to_vec, [0, 1]).to(device)\n","  loss_function = nn.CrossEntropyLoss().to(device)\n","  optimizer = optim.Adam(model.parameters())\n","\n","  train_set = []\n","  yc = Counter()\n","  for x, y in zip(X, Y):\n","    entry = []\n","    yc[y] += 1\n","    entry.append(torch.LongTensor([y]))\n","    entry.append(make_doc_vector(x, model.word_to_ix))\n","    train_set.append(entry)\n","  print(yc)\n","  data = DataLoader(train_set, batch_size=16, shuffle=True, collate_fn=generate_batch)\n","\n","  for epoch in range(EPOCHS):\n","    train_loss, train_acc = 0, 0\n","    print(\"Epoch: %d\" % epoch)\n","    for _, (x, offsets, y) in enumerate(data):\n","      model.zero_grad()\n","      x, offsets, y = x.to(device), offsets.to(device), y.to(device)\n","      pred = model(x, offsets)\n","      loss = loss_function(pred, y)\n","      loss.backward()\n","      optimizer.step()\n","      train_loss += loss.item()\n","      train_acc += (pred.argmax(1) == y).sum().item()\n","    print(\"Loss: %g, Acc: %g\" % (train_loss / len(train_set), train_acc / len(train_set)))\n","  return model"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"error","timestamp":1713627926847,"user_tz":-480,"elapsed":334099,"user":{"displayName":"陳品伃","userId":"10060191056859703941"}},"outputId":"48c0487b-e52e-4349-bd0d-6f064ae85f1e","id":"vwonHyBcJRWu"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["[===================-------------------------------] 38.7% 644.1/1662.8MB downloaded"]}]},{"cell_type":"markdown","source":["### GPU T4"],"metadata":{"id":"ZqS84cDCHzlb"}},{"cell_type":"code","metadata":{"id":"Aek5tb3yulJb","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3797818b-13ca-4f5b-aeee-0370b518d08e","executionInfo":{"status":"ok","timestamp":1713628119421,"user_tz":-480,"elapsed":10254,"user":{"displayName":"陳品伃","userId":"10060191056859703941"}}},"source":["model = build_model(train_X, train_Y)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Counter({1: 803, 0: 775})\n","Epoch: 0\n","Loss: 0.0427771, Acc: 0.576679\n","Epoch: 1\n","Loss: 0.0338994, Acc: 0.76109\n","Epoch: 2\n","Loss: 0.0235468, Acc: 0.867554\n","Epoch: 3\n","Loss: 0.0145542, Acc: 0.930292\n","Epoch: 4\n","Loss: 0.0080601, Acc: 0.967681\n","Epoch: 5\n","Loss: 0.00418957, Acc: 0.989227\n","Epoch: 6\n","Loss: 0.00208948, Acc: 0.996831\n","Epoch: 7\n","Loss: 0.00108052, Acc: 0.998733\n","Epoch: 8\n","Loss: 0.000602972, Acc: 0.999366\n","Epoch: 9\n","Loss: 0.000370786, Acc: 1\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aT85z26VAInH","outputId":"b22048e3-5d47-4e5c-b1aa-2c94158c6693","executionInfo":{"status":"ok","timestamp":1713628119421,"user_tz":-480,"elapsed":12,"user":{"displayName":"陳品伃","userId":"10060191056859703941"}}},"source":["def predict(model, document):\n","  probs = model(make_doc_vector(document, model.word_to_ix).to(device), torch.tensor([0]).cumsum(dim=0).to(device))\n","  return int(torch.argmax(probs))\n","\n","print(predict(model, \"this is a uninteresting movie\".split(\" \")))\n","print(predict(model, \"a good movie of this year\".split(\" \")))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0\n","1\n"]}]}]}